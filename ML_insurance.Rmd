---
title: "ML In Insurance"
author: "Filipe Santos"
date: "6 March 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Machine Learning in Insurance

This is a small presentation about Machine learning in insurance using a case study taken from a online competition form the website **Kaggle** <https://www.kaggle.com/c/bnp-paribas-cardif-claims-management>. 

The objective is to share ideas and to incite colegues to explore this field and brainstorm about potential uses in the insurance industry.

***

![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/kaggle_bnp.png)

***

In this competition we where provided with an anonymized dataset containing both categorical and numeric variables available when the claims were received by BNP Paribas Cardif. All string type variables are categorical. There are no ordinal variables.

The "target" column in the train set is the variable to predict. It is equal to 1 for claims suitable for an accelerated approval.

The task is to predict a probability ("PredictedProb") for each claim in the test set.

## Know thy Data.. 
### but do it fast and ugly to move into modelling

On the website we just need to sign in and start dowloading the data into our computer and start hacking into it.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(caret)
library(Matrix)
library(SOAR)
library(Amelia)
library(corrplot)
library(ROCR)
library(gridExtra)

```

Usualy two files are available, the train data, wich is a file with a large amount of data to train our models and a Test file. The test file does not have the target variable and is used to obtain the scores from our model on unobserved data that will be uploaded in to the competition website to get a score for our models ana position on the leaderboard.

Lets start importing the data into R.

```{r import, cache=TRUE, message=FALSE}
train.i <- fread("./data/train.csv")
test.i <- fread("./data/test.csv")
```

a quick look at the dimension of the data sets:

```{r}
cat("\nNumber of Rows in Train: ",nrow(train.i),"Number of columns: ", ncol(train.i))
cat("\nNumber of Rows in Test: ",nrow(test.i),"Number of columns: ", ncol(test.i))
```

...and the Variable names.

```{r}
names(test.i)
names(train.i)
```

Usually the difference between the two data sets is the targer variable.
A nice practice is to put our target variable on a separate dataframe like this:

```{r}
#Response Variable
response_name<-setdiff(names(train.i),names(test.i))
train_target <- train.i[,target]
response_name
```

```{r}
convClaims <- (sum(train.i$target)/nrow(train.i))*100
cat("\nProportion of Claims that were suitable for an accelerated approval: ", convClaims, "%")
```

```{r, message=FALSE}
library(plotly)
library(ggplot2)
table(train.i$target)

gg<-ggplot(data=train.i, aes(target))+ geom_bar()
#gg
```

A look into the proportion of the classes.

```{r}

ggplotly(gg)
```

Now lets clean the target variable from the original data.frame and remove the ID column (please **dont use this** one for modeling), as expected we will not use this one as factor for our predictive models.
```{r, message=FALSE, warning=FALSE, include=FALSE}

# Remove id and Target variable from train data set
train.i[,target:=NULL]

# joing both table to be create factors
all.data<-rbind(as.data.frame(train.i),as.data.frame(test.i))
# Setting Character variables as Factors
character_vars <- lapply(all.data, class) == "character" 
all.data[, character_vars] <- lapply(all.data[, character_vars], as.factor)
all.data$ID<-as.integer(all.data$ID)

#A Separate train and test data
train<-data.table(all.data[1:nrow(train.i),],keep.rownames=TRUE)
test<-data.table(all.data[(nrow(train.i)+1):nrow(all.data),],keep.rownames=TRUE)

#remove id from train
train[,ID:=NULL]
train[,rn:=NULL]
test[,rn:=NULL]
```

We will only use the train data for modeling.

###  Treating Numerical variables 

```{r}
cat("\n\nNumeric variables on the train set....")
train_num <- train[,names(train)[which(sapply(train, is.numeric))], with = FALSE]
cat("\nColumns with Numeric values: ",names(train_num))
#Show some values
str(lapply(train_num, unique), vec.len = 4)
```


```{r}
cat("\nFindind highly correlated variables: " )
descrCor<- cor(train_num, y = NULL, use = "na.or.complete")
summary(descrCor[upper.tri(descrCor)])

```


```{r}
corrplot(descrCor, order = "hclust")
```
removing high correlated variables...

```{r}
highlyCorDescr <- findCorrelation(descrCor, cutoff = .85)
cat("\nHighly correlated variables above 0.85 : ", names(train_num)[highlyCorDescr])
names(train_num)[highlyCorDescr]
```

```{r}
#Remove these variales
train_num_clean<- train_num[, c(highlyCorDescr):=NULL ]
  
descrCor2<- cor(train_num_clean, y = NULL, use = "na.or.complete")
summary(descrCor2[upper.tri(descrCor2)])

```

```{r}
corrplot(descrCor2, order = "hclust")
```


#### Treating Missing Values

For numerical data we will center and scale the variables and use the **mean** as an input for missing data. In the **Caret** package you can easlly use k-nearest neighborhood for inputing data or bag-tree model, just look at the different methods/options on the code.

```{r}
#Preprocess and input missing values
pp.numeric <- preProcess(train_num_clean,method=c("center", "scale","medianImpute"))
#                 method=c("bagImpute"))
#                 method=c("knnImpute"))
train_num.inputed <- predict(pp.numeric,train_num_clean)
```


###  Treatment of Character variables  

```{r}
#Char vars
#cat("\n\nChar variables train & test set....")
train_factor <- train[,names(train)[which(sapply(train, is.factor))], with = FALSE]
#cat("\nColumns with Character values: ",names(train_char))
#Show some values
str(lapply(train_factor, unique), vec.len = 4)
```

```{r}
print("Some factor levels are blanks or whitespaces",quote=F)
print("We can change them to respectively Bl and Ew",quote=F)
fact_var_names<-sapply(train_factor,is.factor)
fact_var_names<-attr(fact_var_names[fact_var_names==T],"names")
replace_lev<-function(x){
  levels(x)[levels(x)==""]<-"Bl"
  levels(x)[levels(x)==" "]<-"Ws"
  return(x)       
}
train_char<-as.data.frame(train_factor)
train_char[,fact_var_names]<-as.data.frame(sapply(train_char[,fact_var_names],replace_lev))
print("Done!",quote=F)
```

```{r}
cat("Factor column count : ", dim(train_char)[2])
```

```{r}
#We can see that 2 factors have to many values, i dint know thw underling structure or ways to agregate this data.class(
#   will remove these two
train_char$v22 <- NULL
train_char$v56 <- NULL
train_char$v125 <- NULL
```
```{r}
str(lapply(train_char, unique), vec.len = 4)
```

### Get all the data together 

```{r}
train.all <- as.data.frame(cbind(train_num.inputed,train_char))
train_target_coded<- ifelse(train_target==1, "yes","no")

train_target_coded<- as.factor(train_target_coded)

table(train_target)
table(train_target_coded)
```


```{r}
cat("Train data has", nrow(train.all), "rows and", ncol(train.all), "columns! \n")
#str(lapply(train.all, unique), vec.len = 4)
```


```{r, include=FALSE}
rm(colCnt.df,train, train_char,train_charv2,train_num,train.nzv,test.i,train_factor, train_num_clean,train.i,train_num.inputed,all.data,train_num.inputed,all.data)
```

###  Lets Talk about the Bias-Variance Trade off
###  ... because its kinda a important topic in Machine Learning

* About Bias-Variance
    + Bias refers to the error that is introduced by approximating
a real-life problem, which may be extremely complicated, by a much
simpler model.
    + Variance refers to the amount by which Ë† f would change if we
estimated it using a different training data set
    

(ref: ISL )


***
![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/trainerror.png)

***

###  Setup for Modeling & Cross-Validation 


```{r}
train_matrix <- as.data.frame(model.matrix( ~ .-1, data = train.all, sparse=FALSE))
train_target_coded <- relevel(train_target_coded, ref = 1)
contrasts(train_target_coded)

cat("Train Matrix has", nrow(train_matrix), "rows and", ncol(train_matrix), "columns! \n")

```

```{r}
set.seed(998)
#CvFoldsForTrain <- createFolds(train_target_coded, k = 6,list = TRUE, returnTrain=TRUE)
#str(CvFoldsForTrain)
trainIndex <- createDataPartition(train_target_coded, p = .8,
                                  list = FALSE,
                                  times = 1)
str(trainIndex)

train_sample <- train_matrix[trainIndex,]
target_sample_coded <- (train_target_coded[trainIndex])

```


We will create here 6 stratified folds of data, the intention is to create diferentes models with these folds and ate the end we will combine the results of the diffrent models, creating our own ensemble model.
```{r}
set.seed(998)
#CvFoldsForTrain <- createFolds(train_target_coded, k = 6,list = TRUE, returnTrain=TRUE)
#str(CvFoldsForTrain)
trainIndex <- createDataPartition(train_target_coded, p = .8,
                                  list = FALSE,
                                  times = 1)
                                  
train_sample <- train_matrix[trainIndex,]
target_sample_coded <- (train_target_coded[trainIndex])
```
***

![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/decisionboudary.png)

***

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           ## k-fold CV...
                           number = 3,
                           ## repeated 3 times
                           repeats = 1,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = mnLogLoss)
```

***

![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/logloss.png)

***



```{r }
MultiLogLoss <- function(act, pred)
    {
      eps = 1e-15;
      nr <- nrow(pred)
      pred = matrix(sapply( pred, function(x) max(eps,x)), nrow = nr)      
      pred = matrix(sapply( pred, function(x) min(1-eps,x)), nrow = nr)
      ll = sum(act*log(pred) + (1-act)*log(1-pred))
      ll = ll * -1/(nrow(act))      
      return(ll);
    }
```


![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/kfold.png)

***

##   Random Forest (RF) 


***
![ensemble](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/ensemble.png)

***

```{r eval=TRUE}
#table(target_sample)
dim(train_sample)
length(target_sample_coded)
prop.table(table(target_sample_coded))
```

```{r, echo=TRUE}
mtryValues <- c(5,as.integer(sqrt(ncol(train.all))),15,35,50)
mtryValues
```

* RForest Parameters 
    + Number of trees
    + Number of random features

```{r eval=FALSE}
set.seed(998)
rfFit <- train(x = train_sample,
               y = target_sample_coded ,
               method = "rf",
               ntree = 185,
               tuneGrid = data.frame(mtry = mtryValues),
               importance = TRUE,
               trControl = fitControl,
               maximize=FALSE,
               metric = "logLoss"
               )
```


```{r}
load('rfFit.rda') 
rfFit
```

```{r}
plot(rfFit)
```

```{r, message=FALSE, warning=FALSE}
importance <- varImp(rfFit, scale=FALSE)
print(importance)
```
Variable importance
```{r}
# plot importance
plot(importance,top = 20)
```

###   Gradient Bosting Trees (GBM) 

***
![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/gradient.png)

***


* GBM Parameters 
    + trees depth
    + boosting iterations
    + shrinkage
    + minimum of instances on the trees leaf
       

```{r}
gbmGrid <-  expand.grid(interaction.depth = c(5,10,15),
                        n.trees = (1:4)*75,
                        shrinkage = 0.1,
                        n.minobsinnode = c(20))
gbmGrid
```


```{r eval=FALSE}
set.seed(825)
gbmFit <- train(x = train_sample,
                y = target_sample_coded ,
                 method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE,
                 tuneGrid = gbmGrid,
                 ## Specify which metric to optimize
                 maximize=FALSE,
                 metric = "logLoss")
```


```{r}
load('gbmFit.rda') 
gbmFit
```

```{r}
plot(gbmFit)
```


###   Artificial Neural Networks (ANN) 
***

![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/ANN.png)

***

* ANN Parameters 
    + number of Hiden units on the hidden layer
    + wheight decay
 

```{r}
nnetGrid <- expand.grid(.size = c(2,4,6,12), .decay = c(0,0.1,0.5,1))
nnetGrid
```

```{r}
maxSize <- max(nnetGrid$.size)
numWts <- 1*(maxSize * (length(target_sample_coded) + 1) + maxSize + 1)
#number of parameters of the ANN
numWts
```

```{r eval=FALSE}
set.seed(998)
nnetFit <- train(y = target_sample_coded,
                 x= train_sample,
                 method = "nnet",
                 #preProc = c("center", "scale"),
                 tuneGrid = nnetGrid,
                 trace = FALSE,
                 maxit = 2000,
                 MaxNWts = numWts,
                 ## ctrl was defined
                 trControl = fitControl,
                 maximize=FALSE,
                 metric = "logLoss")
```

```{r, include=FALSE}
load('nnetFit.rda')
```

```{r}
nnetFit
plot(nnetFit)
```



###   Support Vector Machines (SVM)  
***

#![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/svm.png)

\includegraphics[width=850pt,height=750pt]{/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/svm.png}

***


* SVM Parameters 
    + sigma
    + cost
    

```{r}
svmRGridReduced <- expand.grid(.sigma = c(0.01, 0.05,0.1),
                    .C = c(0.75, 1, 1.25))
svmRGridReduced
```

```{r eval=FALSE}
set.seed(998)
svmRModel <- train(y= target_sample_coded,
                   x= train_sample ,
                   method = "svmRadial", 
                   tuneGrid = svmRGridReduced, 
                   trControl = fitControl,
                   maximize=FALSE,
                   metric = "logLoss")
```

```{r}
load('svmRModel.rda')
```

```{r}
svmRModel
plot(svmRModel)
```

###   K-Nearest Neighbours (KNN)

***
![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/kNN.png)

***

* KNN Parameters 
    + number of neighbours

```{r}
knnGrid <- data.frame(.k = c(as.integer(sqrt(nrow(train_sample)))))
knnGrid
```

```{r eval=FALSE}
set.seed(998)
knnFit <- train(y = target_sample_coded,
                x= train_sample,
                method = "knn",
                tuneGrid = knnGrid,
                trControl = fitControl,
                maximize=FALSE,
                metric = "logLoss")
```

```{r}
load('knnFit.rda')
```

```{r}
knnFit
```

###   Comparing models 

```{r}
results <- resamples(list(RF=rfFit, GBM=gbmFit, SVM=svmRModel, KNN=knnFit, ANN=nnetFit))
# summarize the distributions
summary(results)
```
```{r}
# boxplots of results
bwplot(results)
```

###   Model Staking 


```{r, eval=FALSE}
# RForest
trainClasses.rf  <- predict(rfFit, newdata = train_sample,type = "prob")
# GBM
trainClasses.gbm <- predict(gbmFit, newdata = train_sample ,type = "prob")
# ANN
trainClasses.ann <- predict(nnetFit, newdata = train_sample ,type = "prob")
# SVM
trainClasses.svm <- predict(svmRModel, newdata = train_sample ,type = "prob")
# KNN
trainClasses.knn <- predict(knnFit, newdata = train_sample ,type = "prob")
# ANN
trainClasses.ann <- predict(nnetFit, newdata = train_sample ,type = "prob")
```

```{r, eval=FALSE}
model_probs_stack <- data.frame(RForest=trainClasses.rf[2] , GBM=trainClasses.gbm[2] , ANN=trainClasses.ann[2], SVM=trainClasses.svm[2] , KNN= trainClasses.knn[2])

names(model_probs_stack)<- c("RForest","GBM","ANN","SVM","KNN")
```

```{r}
load('model_probs_stack.rda')
head(model_probs_stack,10)
```

```{r}
set.seed(998)
fitStaking <- trainControl("cv", 5, 
                           savePredictions = FALSE, 
                           classProbs = TRUE,
                            summaryFunction = twoClassSummary)
```

```{r, eval=FALSE}
lrFull <- train(y = stack_target_sample_coded,
                x= model_probs_stack,
              method = "glm", trControl = fitStaking,
              family = binomial, 
              metric = 'ROC', 
              trace = FALSE)
```

```{r}
load('model_probs_stack.rda')
load('lrFull.rda') 
lrFull
```

```{r}
lrFull$finalModel$coefficients
```

```{r}
data_pred<- predict(lrFull, newdata = model_probs_stack)

confusionMatrix(data = data_pred, reference = target_sample_coded,positive="yes")
```


### ---------  Test Models ------------

```{r}
test_sample <- train_matrix[-trainIndex,]
target_test_coded <- (train_target_coded[-trainIndex])
```

Predict with all models created

```{r, eval=FALSE}
# RForest
p.rf  <- predict(rfFit, newdata = test_sample ,type = "prob")[2]
#GBM
p.gbm  <- predict(gbmFit, newdata = test_sample,type = "prob")[2]
#ANN
p.ann  <- predict(nnetFit, newdata = test_sample,type = "prob")[2]
#SVM
p.svm  <- predict(svmRModel, newdata = test_sample,type = "prob")[2]
#KNN
p.knn  <- predict(knnFit, newdata = test_sample,type = "prob")[2]
```

```{r, eval=FALSE}
test_model_probs <- data.frame(RForest=p.rf,GBM=p.gbm,ANN= p.ann,SVM=p.svm,KNN= p.knn)
names(test_model_probs)<- c("RForest","GBM","ANN","SVM","KNN")
```

```{r, include=FALSE}
load('test_model_probs.rda')
p.rf<-test_model_probs$RForest
p.gbm<-test_model_probs$GBM
p.ann<-test_model_probs$ANN
p.svm<-test_model_probs$SVM
p.knn<-test_model_probs$KNN
```


predicted stacked model (generalized model staking)
```{r}
p.ensemble<-predict(lrFull, newdata = test_model_probs,type = "prob")[2]
```


```{r}


library(ROCR)
library(gridExtra)
proc <- function(pr.m)
{
pr <- prediction(pr.m, target_test_coded)
pe <- performance(pr, "tpr", "fpr")
au <- performance(pr, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle(deparse(substitute(pr.m)))
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
                  label=paste("AUC =", round(au, 2)))
return(p)
}
```

```{r,cache=TRUE, fig.height=5, fig.width=9}
grid.arrange(proc(p.rf),proc(p.gbm), proc(p.ann),proc(p.svm),proc(p.knn), proc(p.ensemble),ncol=3)
```

```{r}

proc_pre_recall <- function(pr.m)
{
  pr <- prediction(pr.m, target_test_coded)
  pe <- performance(pr, "prec", "rec")
  au <- performance(pr, "auc")@y.values[[1]]
  pd <- data.frame(rec=unlist(pe@x.values), prec=unlist(pe@y.values))
  p <- ggplot(pd, aes(x=rec, y=prec))
  p <- p + geom_line(colour="red")
  p <- p + xlab("Recall") + ylab("Precision")
  p <- p + ggtitle(deparse(substitute(pr.m)))
  #p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
  #                  label=paste("AUC =", round(au, 2)))
  return(p)
}
```

```{r,cache=TRUE, fig.height=5, fig.width=9}
grid.arrange(proc_pre_recall(p.rf),proc_pre_recall(p.gbm), proc_pre_recall(p.ann),proc_pre_recall(p.svm),proc_pre_recall(p.knn), proc_pre_recall(p.ensemble),ncol=3)

```


# Bibliography 


Sensitivity: given that a result is truly an event, what is the
probability that the model will predict an event results?
Specificity: given that a result is truly not an event, what is the
probability that the model will predict a negative results?


***
![](/Users/macintosh/Desktop/Kaggle Competitions/BNP Paribas Cardif Claims Management/images/conf_matrix.png)

***
